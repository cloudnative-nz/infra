# -*- org-return-follows-link: t; -*-
#+TITLE: uk.sharing.io
#+AUTHOR: Hippie Hacker
#+EMAIL: hh@ii.coop
#+DATE: 28th of Augest, 2023
#+PROPERTY: header-args:bash+ :results replace verbatim code output
#+PROPERTY: header-args:bash+ :var SPACE_TLD=(symbol-value 'space-domain)
#+NOPROPERTY: header-args:bash+ :var TSOCKET=(symbol-value 'tmux-socket)
#+PROPERTY: header-args:bash+ :dir (symbol-value 'tramp-dir)
#+PROPERTY: header-args:bash+ :wrap example
#+PROPERTY: header-args:bash+ :async
#+PROPERTY: header-args:shell+ :results replace verbatim code output
#+PROPERTY: header-args:shell+ :var SPACEDOMAIN=(symbol-value 'space-domain)
#+PROPERTY: header-args:shell+ :var KUBECONFIG=(concat (getenv "HOME") "/.kube/config-" space-domain)
#+NOPROPERTY: header-args:shell+ :var TSOCKET=(symbol-value 'tmux-socket)
#+PROPERTY: header-args:shell+ :async
#+NOPROPERTY: header-args:tmux+ :session "io:ssh"
#+NOPROPERTY: header-args:tmux+ :socket (symbol-value 'tmux-socket)
#+NOSTARTUP: content
#+NOSTARTUP: overview
#+NOSTARTUP: show2levels
#+STARTUP: showeverything
* WWW
** Who?
ii.nz team
** What?
Configure an Equinix Provided [[https://deploy.equinix.com/developers/docs/metal/hardware/standard-servers/#m3largex86][m3.large.x86]] into a single node kubernetes cluster.
** Why?
We want to serve up fast and local services for our cloud native friends in Europe.
** Where?
London [[https://deploy.equinix.com/developers/docs/metal/locations/metros/#europe-and-middle-east][Equinix Facility]]
** How?
Deploying Ubuntu + Kubernetes + flux + gitops + coder + some templates.
* Inventory
** Computer with Ubuntu
In this case we created one using the Equinix [[https://deploy.equinix.com/developers/docs/metal/libraries/cli/][metal cli]].
*** device creation
:PROPERTIES:
:header-args:shell+: :var KUBECONFIG="/Users/hh/.kube/config-sharing.io"
:header-args:shell+: :var CODER_CONFIG_DIR="/Users/hh/.config/coder.sharing.io"
:header-args:tmux+: :session ":creation"
:END:
**** create
#+begin_src shell :wrap "src yaml"
metal device create \
    --metro ld \
    --plan m3.large.x86 \
    --operating-system ubuntu_22_04 \
    --hostname cp1.uk.sharing.io \
    --output yaml \
    cp1.uk.sharing.io
#+end_src

#+RESULTS:
#+begin_src yaml
billing_cycle: hourly
created_at: "2023-08-28T13:31:26Z"
created_by:
  avatar_thumb_url: https://www.gravatar.com/avatar/88099dd30103c7cc4ab013e4aa9b2c83?d=mm
  created_at: "2017-08-15T16:22:11Z"
  email: packet@cncf.io
  first_name: CNCF
  full_name: CNCF Team
  href: /metal/v1/users/19d434d9-ca19-4230-928e-ea3115a3e859
  id: 19d434d9-ca19-4230-928e-ea3115a3e859
  last_name: Team
  level: verified
  short_id: 19d434d9
  updated_at: "2023-08-28T13:31:27Z"
facility:
  address:
    address: 1 Banbury Ave
    city: Slough
    coordinates: {}
    country: GB
    id: cc6637b9-c140-42b4-89dc-03e98f255f37
    zip_code: SL1 4LH
  code: ld7
  features:
  - baremetal
  - backend_transfer
  - layer_2
  - global_ipv4
  - ibx
  id: 8002e6d0-737a-47c5-85a1-092fedced9f1
  metro:
    code: ld
    country: GB
    id: 5f1d3910-2059-4737-8e7d-d4907cfbf27f
    name: London
  name: London
hostname: cp1.uk.sharing.io
href: /metal/v1/devices/afbf4b93-a21d-4581-8cc4-7b3327acb141
id: afbf4b93-a21d-4581-8cc4-7b3327acb141
ip_addresses: []
metro:
  code: ld
  country: GB
  id: 5f1d3910-2059-4737-8e7d-d4907cfbf27f
  name: London
network_ports:
- bond:
    id: fb42374b-6bdd-47f1-92ca-f8cc2da07278
    name: bond0
  data:
    bonded: true
    mac: b4:96:91:e6:d5:18
  disbond_operation_supported: true
  href: /metal/v1/ports/2acb95a5-510f-4f38-9523-da638e714bf7
  id: 2acb95a5-510f-4f38-9523-da638e714bf7
  name: eth0
  type: NetworkPort
- bond:
    id: fb42374b-6bdd-47f1-92ca-f8cc2da07278
    name: bond0
  data:
    bonded: true
    mac: b4:96:91:e6:d5:19
  disbond_operation_supported: true
  href: /metal/v1/ports/d4c75505-d52f-462c-b353-646da9272b7c
  id: d4c75505-d52f-462c-b353-646da9272b7c
  name: eth1
  type: NetworkPort
- data:
    bonded: true
  disbond_operation_supported: true
  href: /metal/v1/ports/fb42374b-6bdd-47f1-92ca-f8cc2da07278
  id: fb42374b-6bdd-47f1-92ca-f8cc2da07278
  name: bond0
  network_type: layer2-bonded
  type: NetworkBondPort
operating_system:
  distro: ubuntu
  name: Ubuntu 22.04 LTS
  slug: ubuntu_22_04
  version: "22.04"
plan:
  class: m3.large.x86
  description: Our m3.large.x86 server is ideal for virtualization, with an AMD EPYC
    7502P 32C/64T processor @ 2.5GHz and 256GB RAM
  id: cb6a01ea-0120-4a59-ad6a-bcc14d8bf487
  line: baremetal
  name: m3.large.x86
  slug: m3.large.x86
  specs:
    cpus:
    - count: 1
      type: AMD EPYC 7513 32-Core Processor @ 2.6Ghz
    drives:
    - count: 2
      size: 256GB
      type: NVME
    - count: 2
      size: 3.8TB
      type: NVME
    features:
      raid: true
      txt: true
    memory:
      total: 256GB
    nics:
    - count: 2
      type: 25Gbps
project:
  backend_transfer_enabled: false
  href: /metal/v1/projects/f4a7273d-b1fc-4c50-93e8-7fed753c86ff
provisioning_events:
- body: Provisioning started
  interpolated: Provisioning started
  type: provisioning.101
- body: Network configured
  interpolated: Network configured
  type: provisioning.102
- body: Configuration written, restarting device
  interpolated: Configuration written, restarting device
  type: provisioning.103
- body: Connected to magic install system
  interpolated: Connected to magic install system
  type: provisioning.104
- body: OS image retrieved
  interpolated: OS image retrieved
  type: provisioning.104.50
- body: Server partitions created
  interpolated: Server partitions created
  type: provisioning.105
- body: Operating system packages installed
  interpolated: Operating system packages installed
  type: provisioning.106
- body: Server networking interfaces configured
  interpolated: Server networking interfaces configured
  type: provisioning.107
- body: Cloud-init packages installed and configured
  interpolated: Cloud-init packages installed and configured
  type: provisioning.108
- body: Installation finished, rebooting server
  interpolated: Installation finished, rebooting server
  type: provisioning.109
- body: Device phoned home and is ready to go
  interpolated: Device phoned home and is ready to go
  type: provisioning.110
short_id: afbf4b93
ssh_keys:
- Owner:
    href: ""
  created_at: ""
  fingerprint: ""
  href: /metal/v1/ssh-keys/065f03dd-b58e-4f88-832d-e200917becb9
  id: ""
  key: ""
  label: ""
  updated_at: ""
state: queued
switch_uuid: 025c38d8
updated_at: "2023-08-28T13:31:27Z"
user: root
volumes: []
#+end_src

**** ip
#+name: ip
#+begin_src shell
metal device get --filter hostname=cp1.uk.sharing.io -o json | jq '.[0].ip_addresses[0].address' -r
#+end_src

#+RESULTS: ip
#+begin_example
145.40.113.253
#+end_example
**** id
#+name: id
#+begin_src shell :sync
metal device get --filter hostname=cp1.uk.sharing.io -o json | jq -r '.[0].id'
#+end_src

#+RESULTS: id
#+begin_example
afbf4b93-a21d-4581-8cc4-7b3327acb141
#+end_example
**** facility
#+name: facility
#+begin_src shell :sync
metal device get --filter hostname=cp1.uk.sharing.io -o json | jq -r '.[0].facility.code'
#+end_src

#+RESULTS: facility
#+begin_example
ld7
#+end_example

**** serial bios / hardware console access
:PROPERTIES:
:header-args:shell+: :var ID=eval-block("id")
:header-args:shell+: :var FACILITY=eval-block("facility")
# :header-args:tmux+: :prologue (concat "ID=" eval-block("id") "\nFACILITY=" eval-block("facility") "\n")
:END:

This will allow us to watch for any errors that come up, but usually is uneventful, just giving us an indecation of how far along we are.
#+name: ssh_sos
#+begin_src shell :var ID=eval-block("id") :var FACILITY=eval-block("facility") :wrap "src tmux :session \":ssh\"" :sync
echo ssh $ID@sos.$FACILITY.platformequinix.com
#+end_src

#+RESULTS: ssh_sos
#+begin_src tmux :session ":ssh"
ssh afbf4b93-a21d-4581-8cc4-7b3327acb141@sos.ld7.platformequinix.com
#+end_src


** Domain
NS Records and a separate tsig key are configured on powerdns.ii.nz
#+name: spacedomain
#+begin_src bash :wrap "example" :sync :cache yes
echo -n $SPACE_TLD
#+end_src

#+RESULTS[485cd51712e0b43d1bdb7e411adcf09f81348bcb]: spacedomain
#+begin_example
uk.sharing.io
#+end_example

* Connect domain to our IP
** What is our IP?
#+begin_src shell :sync
metal device get --filter hostname=cp1.uk.sharing.io -o json | jq '.[0].ip_addresses[0].address' -r
#+end_src

#+RESULTS:
#+begin_example
145.40.113.253
#+end_example

** Add DOMAIN -> Address pointing our IP
We did this via the GUI, but here is the verified result.
#+name: add main A record
#+begin_src shell :sync
dig A $SPACE_TLD +short @ns.ii.nz
#+end_src

#+RESULTS: add main A record
#+begin_example
145.40.113.253
#+end_example

** Add *.DOMAIN -> Address pointing to our IP
#+name: add wildcard A record
#+begin_src shell :sync
dig A random123.$SPACE_TLD +short
#+end_src

#+RESULTS: add wildcard A record
#+begin_example
145.40.113.253
#+end_example
* Verify DNS + SSH Connectivity
** ssh root@uk.sharing.io
You should be able to login with your password (or ssh key)
#+begin_src tmux :prologue (concat "export SPACE_TLD=" space-domain "\n")
ssh root@cp1.$SPACE_TLD
#+end_src
** ssh-import-id to ensure Hippie, Stephen, and Zach Have access
#+begin_src tmux
ssh-import-id gh:hh gh:iiamabby gh:heyste gh:zachmandeville gh:iiamabby
#+end_src
* install
** trust packages from google, kubernetes, and docker
#+begin_src tmux
curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg \
    | gpg --dearmor -o /etc/apt/trusted.gpg.d/google.gpg
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key \
    | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/kubernetes-apt-keyring.gpg
curl -fsSL https://download.docker.com/linux/ubuntu/gpg \
    | gpg --dearmor -o /etc/apt/trusted.gpg.d/docker.gpg
#+end_src
** add repos from docker and googl
#+begin_src tmux
apt-add-repository "deb https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /" -y
add-apt-repository "deb https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" -y
apt-add-repository "deb http://packages.cloud.google.com/apt cloud-sdk main" -y
#+end_src
** ttyd tmux curl containerd
#+begin_src tmux
DEBIAN_FRONTENT=noninteractive apt-get install -y \
    -o Dpkg::Options::="--force-confdef" \
    -o Dpkg::Options::="--force-confold"  \
    ttyd \
    tmux \
    kitty-terminfo \
    containerd.io \
    curl \
    docker-ce \
    docker-ce-cli \
    kubelet \
    kubeadm \
    open-iscsi \
    nfs-common
#+end_src

** cilium
#+begin_src tmux
sudo su -
cd /tmp
curl -L --remote-name-all \
    https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-amd64.tar.gz.sha256sum
tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin
rm cilium-linux-amd64.tar.gz cilium-linux-amd64.tar.gz.sha256sum
#+end_src
** flux
#+begin_src tmux
curl -s https://fluxcd.io/install.sh | bash
#+end_src
** helm
#+begin_src tmux
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
#+end_src
* configure starting kubernetes
** swap
*** disable swap
#+begin_src tmux
sudo swapoff -a
#+end_src
*** remove swap from /etc/fstab
Swap will be remounted when we reboot, unless we remove it from the File System TAB.
#+begin_src tmux
sudo sed -i '/swap/d' /etc/fstab
#+end_src
*** Check results
Swap will be remounted when we reboot, unless we remove it from the File System TAB.
#+begin_src tmux
free -m
cat /etc/fstab
#+end_src
** containerd
Kubernetes needs systemdcgroup when using cilium
*** [[/ssh:root@uk.sharing.io:/etc/containerd/config.toml][/etc/containerd/config.toml]]
#+begin_src toml :tangle (concat tramp-dir "etc/containerd/config.toml")
version = 2
[plugins]
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
    runtime_type = "io.containerd.runc.v2"
    [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
      SystemdCgroup = true
#+end_src
*** restart containerd w/ new config
#+begin_src bash :results silent :sync
sudo systemctl restart containerd
#+end_src
** [[/ssh:root@uk.sharing.io:/etc/crictl.yaml][/etc/crictl.yaml]]
crictl needs to be confugured to use our containred socket. (It complains otherwise)
#+begin_src toml :tangle (concat tramp-dir "etc/crictl.yaml")
runtime-endpoint: unix:///var/run/containerd/containerd.sock
image-endpoint: unix:///var/run/containerd/containerd.sock
timeout: 10
debug: false
#+end_src
** [[/ssh:root@uk.sharing.io:/etc/kubernetes/kubeadm-config.yaml][/etc/kubernetes/kubeadm-config.yaml]]
*** Default Config
#+begin_src bash :wrap "src yaml" :sync
kubeadm config print init-defaults
#+end_src

#+RESULTS:
#+begin_src yaml
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 1.2.3.4
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  name: node
  taints: null
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.k8s.io
kind: ClusterConfiguration
kubernetesVersion: 1.28.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
#+end_src

*** My InitConfiguration
We need to disabled kube-proxy, and ensure we use the criSocket.
We will let cilium handle the kube-proxy aspects of the cluster
#+begin_src toml :tangle (concat tramp-dir "etc/kubernetes/kubeadm-config.yaml") :comments no
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
skipPhases:
  - addon/kube-proxy
nodeRegistration:
  taints: []
#+end_src
*** My ClusterConfiguration
Let's be sure our naming is specific to this cluster for Certs and DNS
#+begin_src toml :tangle (concat tramp-dir "etc/kubernetes/kubeadm-config.yaml") :comments no
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
clusterName: uk.sharing.io
kubernetesVersion: 1.28.1
controlPlaneEndpoint: "k8s.uk.sharing.io:6443"
apiServer:
  certSans:
    - "145.40.113.253"
    - "k8s.uk.sharing.io"
#+end_src
** [[/ssh:root@uk.sharing.io:/etc/kubernetes/cilium-values.yaml][/etc/kubernetes/cilium-values.yaml]]
These are the helm chart values for the 'kubeproxy-free' setup of Cilium
- [[https://docs.cilium.io/en/latest/network/kubernetes/kubeproxy-free/#quick-start][KubeProxy free Quickstart]]
- [[https://github.com/cilium/cilium/tree/v1.13.3/install/kubernetes/cilium#values][Cilium Helm Values Documentation]]
*** base config
#+begin_src yaml :tangle (concat tramp-dir "etc/kubernetes/cilium-values.yaml")
k8sServiceHost: k8s.uk.sharing.io
k8sServicePort: 6443
kubeProxyReplacement: strict
policyEnforcementMode: "never"
operator:
  replicas: 1
#+end_src
*** Enable Gateway API
I hear this is cool
#+begin_src yaml :tangle (concat tramp-dir "etc/kubernetes/cilium-values.yaml")
gatewayAPI:
  enabled: true
#+end_src
*** (dis)able IngressController
I'm really keen to try this out, but we need to find a way to set the following on the cilium-ingress:
#+begin_src yaml
externalIPs:
  - 145.40.113.253
loadBalancerIP: 145.40.113.253
#+end_src
Along with figuring out connectivity.
#+begin_src yaml :tangle (concat tramp-dir "etc/kubernetes/cilium-values.yaml")
ingressController:
  enabled: false
  service:
    # type: NodePort
    type: LoadBalancer
#+end_src
*** hubble
#+begin_src yaml :tangle (concat tramp-dir "etc/kubernetes/cilium-values.yaml")
hubble:
  enabled: true
  listenAddress: ":4244"
  metrics:
    enabled:
      - dns
      - drop
      - tcp
      - flow
      - port-distribution
      - icmp
      - http
  relay:
    enabled: true
  ui:
    enabled: true
#+end_src
* configure completion
#+begin_src tmux
helm completion bash > /etc/bash_completion.d/helm
flux completion bash > /etc/bash_completion.d/flux
kubectl completion bash > /etc/bash_completion.d/kubectl
#+end_src
* actually init and start kubernetes
** Pull down kubernetes container images
#+begin_src bash :sync
kubeadm config images pull
#+end_src

#+RESULTS:
#+begin_example
[config/images] Pulled registry.k8s.io/kube-apiserver:v1.28.1
[config/images] Pulled registry.k8s.io/kube-controller-manager:v1.28.1
[config/images] Pulled registry.k8s.io/kube-scheduler:v1.28.1
[config/images] Pulled registry.k8s.io/kube-proxy:v1.28.1
[config/images] Pulled registry.k8s.io/pause:3.9
[config/images] Pulled registry.k8s.io/etcd:3.5.9-0
[config/images] Pulled registry.k8s.io/coredns/coredns:v1.10.1
#+end_example

** Inspect kubernetes container images
#+begin_src bash :sync
sudo crictl images
#+end_src

#+RESULTS:
#+begin_example
IMAGE                                     TAG                 IMAGE ID            SIZE
registry.k8s.io/coredns/coredns           v1.10.1             ead0a4a53df89       16.2MB
registry.k8s.io/etcd                      3.5.9-0             73deb9a3f7025       103MB
registry.k8s.io/kube-apiserver            v1.28.1             5c801295c21d0       34.6MB
registry.k8s.io/kube-controller-manager   v1.28.1             821b3dfea27be       33.4MB
registry.k8s.io/kube-proxy                v1.28.1             6cdbabde3874e       24.6MB
registry.k8s.io/kube-scheduler            v1.28.1             b462ce0c8b1ff       18.8MB
registry.k8s.io/pause                     3.9                 e6f1816883972       322kB
#+end_example

** Initialize our cluster
#+begin_src tmux
kubeadm init --config /etc/kubernetes/kubeadm-config.yaml
#+end_src
** Configure our KUBECONFIG
#+begin_src tmux
mkdir -p $HOME/.kube
sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
#+end_src
** wait for apiserver and untaint control plane
I don't think we need this anymore
#+begin_src tmux
until kubectl get --raw='/readyz?verbose'; do sleep 5; done
echo kubectl taint nodes --all node-role.kubernetes.io/control-plane:NoSchedule-
#+end_src
** Gateway API
- https://isovalent.com/blog/post/tutorial-getting-started-with-the-cilium-gateway-api/
Looks like there's a new version at:
- https://gateway-api.sigs.k8s.io/guides/#install-standard-channel

#+begin_src tmux
kubectl apply -f  \
    https://github.com/kubernetes-sigs/gateway-api/releases/download/v0.7.1/experimental-install.yaml
#+end_src

#+RESULTS:
#+begin_example
customresourcedefinition.apiextensions.k8s.io/gatewayclasses.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/gateways.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/httproutes.gateway.networking.k8s.io created
customresourcedefinition.apiextensions.k8s.io/referencegrants.gateway.networking.k8s.io created
namespace/gateway-system created
validatingwebhookconfiguration.admissionregistration.k8s.io/gateway-api-admission created
service/gateway-api-admission-server created
deployment.apps/gateway-api-admission-server created
serviceaccount/gateway-api-admission created
clusterrole.rbac.authorization.k8s.io/gateway-api-admission created
clusterrolebinding.rbac.authorization.k8s.io/gateway-api-admission created
role.rbac.authorization.k8s.io/gateway-api-admission created
rolebinding.rbac.authorization.k8s.io/gateway-api-admission created
job.batch/gateway-api-admission created
job.batch/gateway-api-admission-patch created
#+end_example

** cni: cilium
You may need to run this a few times, and make sure gateway-system is up before you start.
#+begin_src tmux
helm repo add cilium https://helm.cilium.io/
helm upgrade --install cilium cilium/cilium \
    --version 1.14.1 \
    --namespace kube-system \
    -f /etc/kubernetes/cilium-values.yaml
#+end_src
** wait for our node to be Ready
Cluster should be up at this point
#+begin_src tmux
kubectl wait --for=condition=Ready \
    --selector=node-role.kubernetes.io/control-plane="" \
    --timeout=120s node
#+end_src
** copy our kubeconfig local
#+begin_src shell :sync :results silent
scp root@$SPACE_TLD:/etc/kubernetes/admin.conf $KUBECONFIG
#+end_src

** increase maxPods
Our nodes usually run a lot of pods, so the default of 110 is way to low, so we bump it by a magnitude of roughly ten.

https://prefetch.net/blog/2018/02/10/the-kubernetes-110-pod-limit-per-node/

It needs to be set in the kublet config file:
https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/

Ideally we set this via kubeadm, but for now manually add it and restart kubelet.
#+begin_src shell :eval never
echo maxPods: 1024 >> /var/lib/kubelet/config.yaml
#+end_src

#+begin_src bash :sync
grep maxPods /var/lib/kubelet/config.yaml
#+end_src

#+RESULTS:
#+begin_example
maxPods: 1024
#+end_example

#+begin_src bash :eval never
systemctl restart kubelet
#+end_src

#+begin_src bash :sync
kubectl describe nodes  | grep Capacity: -A6
#+end_src

#+RESULTS:
#+begin_example
Capacity:
  cpu:                64
  ephemeral-storage:  242541304Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             263772656Ki
  pods:               1024
#+end_example

** increase fswatch

*** temporary fix
#+begin_src bash
sysctl fs.inotify.max_user_watches=524288
sysctl fs.inotify.max_user_instances=512
#+end_src

*** persistent fix
To make the changes persistent, edit the file /etc/sysctl.d/99-maxnotify.conf and add these lines:
#+begin_src toml :tangle (concat tramp-dir "etc/sysctl.d/99-maxnotify.conf") :comments no
fs.inotify.max_user_watches = 524288
fs.inotify.max_user_instances = 512
#+end_src
You will then need load settings from that file:
#+begin_src shell
sysctl --system
#+end_src
*** verify
#+begin_src bash :sync
sysctl get fs.inotify.max_user_watches
sysctl get fs.inotify.max_user_instances
#+end_src

#+RESULTS:
#+begin_example
fs.inotify.max_user_watches = 524288
fs.inotify.max_user_instances = 512
#+end_example

* Bootstrap Fux + Sops Encryption
** generate a github TOKEN
https://github.com/settings/tokens/new
https://github.com/settings/personal-access-tokens/new
Make sure it's for the right organization
- Administration :: Access: Read and write
- Contents :: Access: Read and write
- Metadata :: Access: Read-only

** setup gh cli and authenticate
#+begin_src tmux
sudo apt-get install gh
#+end_src
** GITHUB_TOKEN
#+begin_src shell :sync
echo export GITHUB_TOKEN=$GITHUB_TOKEN
#+end_src

** Make sure the token has admin access if it needs to add the repo
Otherwise ensure the repo and branch exist, and the token has ability to configure hooks
** bootstrap flux
This needs to be done to the correct folder, owner, and repo...
#+begin_src tmux
flux bootstrap github --branch=uk --owner=sharingio --repository=infra --path=clusters/uk.sharing.io
#+end_src
#+begin_example
? What account do you want to log into? GitHub.com
? What is your preferred protocol for Git operations? HTTPS
? Authenticate Git with your GitHub credentials? Yes
? How would you like to authenticate GitHub CLI?  [Use arrows to move, type to filter]
  Login with a web browser
> Paste an authentication token
#+end_example
** TODO at this point check out the repo and put this file into ./clusters/thinkpad/ or similar
#+begin_src tmux
git clone git@github.com:sharingio/infra || gh repo clone sharingio/infra
cp this.org infra/clusters/NEW/setup.org
#+end_src
** Setup SOPS + Flux
*** install sops binary
**** linux
#+begin_src tmux
wget https://github.com/getsops/sops/releases/download/v3.7.3/sops_3.7.3_amd64.deb
sudo dpkg -i sops_*deb
rm sops_*deb
#+end_src
**** mac
#+begin_src bash
brew install gnupg sops
#+end_src
*** generate gpg key
#+begin_src shell :env KEY_NAME="k8s.uk.sharing.io" :sync :results silent
export KEY_NAME="k8s.uk.sharing.io"
export KEY_COMMENT="flux secrets"

gpg --batch --full-generate-key <<EOF
%no-protection
Key-Type: 1
Key-Length: 4096
Subkey-Type: 1
Subkey-Length: 4096
Expire-Date: 0
Name-Comment: ${KEY_COMMENT}
Name-Real: ${KEY_NAME}
EOF
#+end_src

*** list gpg keys
#+begin_src shell :env KEY_NAME="k8s.uk.sharing.io" :sync
export KEY_NAME="k8s.uk.sharing.io"
gpg --list-secret-keys "${KEY_NAME}"
#+end_src

#+RESULTS:
#+begin_example
sec   rsa4096 2023-08-28 [SCEAR]
      6CFB156EEFE7417E2567D5FD37732CFA004DC229
uid           [ultimate] k8s.uk.sharing.io (flux secrets)
ssb   rsa4096 2023-08-28 [SEAR]

#+end_example

*** import into kubernetes
#+begin_src shell :env KEY_NAME="k8s.uk.sharing.io" :sync
export KEY_NAME="k8s.uk.sharing.io"
export KEY_FP=$(gpg --list-secret-keys "${KEY_NAME}" | grep SCEA -A1 | tail -1 | awk '{print $1}')
kubectl delete secret sops-gpg --namespace=flux-system || true
gpg --export-secret-keys --armor "${KEY_FP}" |
kubectl create secret generic sops-gpg \
--namespace=flux-system \
--from-file=sops.asc=/dev/stdin
#+end_src

#+RESULTS:
#+begin_example
secret/sops-gpg created
#+end_example

*** export key into git
#+begin_src shell :env KEY_NAME="k8s.uk.sharing.io" :results silent :sync
export KEY_NAME="k8s.uk.sharing.io"
export KEY_FP=$(gpg --list-secret-keys "${KEY_NAME}" | grep SCEA -A1 | tail -1 | awk '{print $1}')
# gpg --export --armor "${KEY_FP}" > ./clusters/thinkpad/.sops.pub.asc
gpg --export --armor "${KEY_FP}" > .sops.pub.asc
#+end_src

*** write SOPS config file
#+begin_src shell :env KEY_NAME="k8s.uk.sharing.io" :results silent :sync
export KEY_NAME="k8s.uk.sharing.io"
export KEY_FP=$(gpg --list-secret-keys "${KEY_NAME}" | grep SCEA -A1 | tail -1 | awk '{print $1}')
cat <<EOF >> ./.sops.yaml
creation_rules:
  - path_regex: .*.yaml
    encrypted_regex: ^(data|stringData)$
    pgp: ${KEY_FP}
EOF
#+end_src
* PDNS api Secret
https://github.com/zachomedia/cert-manager-webhook-pdns#powerdns-cert-manager-acme-webhook
Create one here with only access to uk.sharing.io
https://powerdns.ii.nz/admin/manage-keys
** cert-manager
*** create pdns secret
Note that the TSIG_KEY we retrieve is base64 encoded... it get's double encoded as a kubernetes secret. Most places you use a TSIG_KEY are expecting the base64 value we have here.
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :sync :results silent
source .envrc
kubectl -n cert-manager \
    create secret generic pdns \
    --from-literal=api-key="$PDNS_API_KEY" \
    --dry-run=client -o yaml > \
    ./secrets/cert-manager-pdns.yaml
#+end_src

*** encrypt pdns secret
We need to encrypt the pdns-secret with sops and commit/push so flux can decrypt and apply it
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :results silent :sync
sops --encrypt --in-place ./secrets/cert-manager-pdns.yaml
#+end_src

* minio
** Login url + user / password
https://minio.uk.sharing.io/login
Username :  $MINIO_ROOT_USER
Password : $MINI_ROOT_PASSWORD
** mc alias
#+begin_src shell
mc alias set min https://s3.uk.sharing.io $MINIO_ROOT_USER $MINIO_ROOT_PASSWORD
#+end_src

#+RESULTS:
#+begin_example
Added `min` successfully.
#+end_example
** mc mb min
#+begin_src shell
mc mb min/sharingio
#+end_src

#+RESULTS:
#+begin_example
Bucket created successfully `uk/sharingio`.
#+end_example

** mc ls min
#+begin_src shell
mc ls min
#+end_src

#+RESULTS:
#+begin_example
[2023-08-23 10:56:26 BST]     0B sharingio/
#+end_example

** minio-env-config
This will eventually be a file within the minio-pool:
#+begin_src shell
 kubectl -n minio exec pod/minio-pool-0-0 cat /tmp/minio/config.env
#+end_src

*** create minio secret
This is basically a file mapping for an env file called config.env

MINIO_ROOT_USER / MINIO_ACCESS_KEY must be at least 3 characters long... we'll default to "root"

NOTE: Access key length should be at least 3, and secret key length at least 8 characters

#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :sync :results silent
source .envrc
kubectl -n minio \
    create secret generic minio-env-config \
    --from-literal=config.env="export MINIO_ROOT_USER=$MINIO_ROOT_USER
export MINIO_ROOT_PASSWORD=$MINIO_ROOT_PASSWORD
export MINIO_IDENTITY_OPENID_CLIENT_ID=$MINIO_IDENTITY_OPENID_CLIENT_ID
export MINIO_IDENTITY_OPENID_CLIENT_SECRET=$MINIO_IDENTITY_OPENID_CLIENT_SECRET" \
    --dry-run=client -o yaml > \
     ./secrets/minio-env-config.yaml
#+end_src

# export MINIO_ACCESS_KEY=$MINIO_ROOT_USER
# export MINIO_SECRET_KEY=$MINIO_ROOT_PASSWORD" \
*** encrypt minio secret
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :results silent :sync
sops --encrypt --in-place ./secrets/minio-env-config.yaml
#+end_src
*** the secret
It's a single key, but it contains the text of the env.
#+begin_src shell :sync
 kubectl -n minio describe secret minio-env-config
#+end_src

#+RESULTS:
#+begin_example
Name:         minio-env-config
Namespace:    minio
Labels:       kustomize.toolkit.fluxcd.io/name=secrets
              kustomize.toolkit.fluxcd.io/namespace=flux-system
Annotations:  <none>

Type:  Opaque

Data
====
config.env:  319 bytes
#+end_example

** longhorn-minio
*** create minio secret
This is basically a file mapping for an env file called config.env

MINIO_ROOT_USER / MINIO_ACCESS_KEY must be at least 3 characters long... we'll default to "root"

NOTE: Access key length should be at least 3, and secret key length at least 8 characters

#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :sync :results silent
source .envrc
kubectl -n longhorn \
    create secret generic longhorn-minio \
    --from-literal=AWS_ACCESS_KEY_ID="$MINIO_ROOT_USER" \
    --from-literal=AWS_SECRET_ACCESS_KEY="$MINIO_ROOT_PASSWORD" \
    --from-literal=AWS_ENDPOINTS="https://s3.uk.sharing.io" \
    --dry-run=client -o yaml > \
     ./secrets/longhorn-minio.yaml
#+end_src
*** encrypt minio secret
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :results silent :sync
sops --encrypt --in-place ./secrets/longhorn-minio.yaml
#+end_src
*** the secret
Longhorn wants a minio secret to backup to
#+begin_src shell :sync
 kubectl -n longhorn describe secret longhorn-minio
#+end_src

#+RESULTS:
#+begin_example
#+end_example

** OIDC
https://goauthentik.io/integrations/services/minio/
#+begin_src shell
mc mb uk/abcs
#+end_src

#+RESULTS:
#+begin_example
Bucket created successfully `uk/abcs`.
#+end_example

https://min.io/docs/minio/linux/operations/external-iam/configure-openid-external-identity-management.html
* PowerDNS
https://pdns.uk.sharing.io
Username admin
Password $PDNS_API_KEY

** create powerdns secret
This is basically a file mapping for an env file called config.env
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :sync :results silent
source .envrc
# AUTHENTIK_CERT=$(cat "$HOME/Downloads/authentik Self-signed Certificate_certificate.pem")
kubectl -n powerdns \
    create secret generic powerdns \
    --from-literal=api_key=${PDNS_API_KEY} \
    --from-literal=admin_user=${PDNS_ADMIN_USER} \
    --from-literal=admin_email=${PDNS_ADMIN_EMAIL} \
    --from-literal=admin_password=${PDNS_ADMIN_PASSWORD} \
    --from-literal=sql_url=postgresql://postgres:${PDNS_DB_PASSWORD}@pdns-db-postgresql:5432/postgres \
    --from-literal=postgres_password=${PDNS_DB_PASSWORD} \
    --dry-run=client -o yaml > \
     ./secrets/powerdns.yaml
#+end_src
    # --from-literal=authentik_cert=${AUTHENTIK_CERT} \

** encrypt powerdns secret
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :results silent :sync
sops --encrypt --in-place ./secrets/powerdns.yaml
#+end_src

* gitops
** update the gitops.yaml with

Ideally we could store this in a secret... but the helm chart doesn't seem to easily allow that.
flux folks are wanting you to use this to bootstrap and move towards SSO.

#+begin_src shell :sync
echo "$GITOPS_PASSWORD" | gitops get bcrypt-hash
#+end_src

#+RESULTS:
#+begin_example
$2a$10$TWhfTylCxtmd/KwMrPVT/OyfQMFKjruUzios2.SRf6C9/nSRSg.kW
#+end_example

** dashboard
*** ensure GITOPS_PASSWORD is set
#+begin_src bash :sync :dir . :results silent
. .envrc
gitops create dashboard ww-gitops \
  --password=$GITOPS_PASSWORD \
  --export > .../gitops-dashboard.yaml
#+end_src

*** ensure github
#+begin_src bash :sync :dir . :results silent
. .envrc
gitops create dashboard ww-gitops \
  --password=$GITOPS_PASSWORD \
  --export > ./gitops-dashboard.yaml
#+end_src
** reciever
*** setup webhook
https://github.com/cloudnative-nz/infra/settings/hooks/new
**** generate HMAC
#+name: new_hmac
#+begin_src shell :sync
TOKEN=$(head -c 12 /dev/urandom | shasum | cut -d ' ' -f1)
echo export FLUX_RECEIVER_TOKEN=$TOKEN >> .envrc
#+end_src

**** check env
#+begin_src bash :sync :dir . :results silent
. .envrc
echo -n $FLUX_RECEIVER_TOKEN
#+end_src

**** create receiven-token secrets
#+begin_src bash :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :results silent :dir . :sync
. .envrc
KUBECONFIG=~/.kube/config-uk.sharing.io
export KUBECONFIG
kubectl -n flux-system create secret \
    --dry-run=client -o yaml \
    generic receiver-token \
    --from-literal=token=$FLUX_RECEIVER_TOKEN > ./secrets/flux-receiver.yaml
#+end_src
*** encrypt and commit TSIG secret
We need to encrypt the secret with sops and commit/push so flux can decrypt and apply it
#+begin_src bash :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :results silent :dir . :sync
sops --encrypt --in-place ./secrets/flux-receiver.yaml
#+end_src

*** get the ingress
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :dir . :sync
kubectl -n flux-system get ingress webhook-receiver
#+end_src

#+RESULTS:
#+begin_example
NAME               CLASS   HOSTS                        ADDRESS          PORTS     AGE
webhook-receiver   nginx   flux-webhook.uk.sharing.io   145.40.113.253   80, 443   29m
#+end_example

*** get the hook path
We need to encrypt the secret with sops and commit/push so flux can decrypt and apply it
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :dir . :sync
kubectl -n flux-system get receiver
#+end_src

#+RESULTS:
#+begin_example
NAME              AGE   READY   STATUS
github-receiver   29m   True    Receiver initialized for path: /hook/e40123b877be41ea5bf7c2712ebf1fbaec7a0176e1342f60c1848bf8b25b1fbb
#+end_example
*** combine them into something like
Use the Secret and this PayloadURL to create a new webook:
[[https://github.com/sharingio/infra/settings/hooks]]
[[https://github.com/sharingio/infra/settings/hooks/new]]
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :dir . :sync
echo PayloadURL: https://$(kubectl -n flux-system get ingress webhook-receiver -o jsonpath="{.spec.rules[0].host}")$(kubectl -n flux-system get receiver github-receiver -o jsonpath="{.status.webhookPath}")
#+end_src

#+RESULTS:
#+begin_example
PayloadURL: https://flux-webhook.uk.sharing.io/hook/e40123b877be41ea5bf7c2712ebf1fbaec7a0176e1342f60c1848bf8b25b1fbb
#+end_example
*** current hook
** oidc-access
From https://docs.gitops.weave.works/docs/configuration/oidc-access/#configuration
- https://docs.gitops.weave.works/docs/configuration/oidc-access/#scopes
*** create flux-system oidc-auth secret
#+begin_src bash :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :results silent :dir . :sync
. .envrc
KUBECONFIG=~/.kube/config-uk.sharing.io
export KUBECONFIG
kubectl -n flux-system create secret \
    --dry-run=client -o yaml \
    generic oidc-auth \
    --from-literal=issuerURL=https://auth.uk.sharing.io/application/o/gitops/ \
    --from-literal=clientID=$GITOPS_OIDC_CLIENT_ID \
    --from-literal=clientSecret=$GITOPS_OIDC_CLIENT_SECRET \
    --from-literal=redirectURL=https://gitops.uk.sharing.io/oauth2/callback \
    --from-literal=tokenDuration=1h0m0s \
    --from-literal=claimUsername=email \
    --from-literal=claimGroups=groups \
    > ./secrets/gitops-oidc.yaml
#+end_src
*** encrypt flux-system oidc-auth secret
We need to encrypt the secret with sops and commit/push so flux can decrypt and apply it
#+begin_src bash :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :results silent :dir . :sync
sops --encrypt --in-place ./secrets/gitops-oidc.yaml
#+end_src

* authentic-env
https://sso.uk.sharing.io/
Username "akadmin"
Password $AUTHENTIK_BOOTSTRAP_PASSWORD
** secret
*** create authentik-env secret
This is basically a file mapping for an env file called config.env
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :sync :results silent
source .envrc
kubectl -n authentik \
    create secret generic authentik-env \
    --from-literal=AUTHENTIK_BOOTSTRAP_PASSWORD=$AUTHENTIK_BOOTSTRAP_PASSWORD \
    --from-literal=AUTHENTIK_BOOTSTRAP_TOKEN=$AUTHENTIK_BOOTSTRAP_TOKEN \
    --from-literal=AUTHENTIK_SECRET_KEY=$AUTHENTIK_SECRET_KEY \
    --dry-run=client -o yaml > \
     ./secrets/authentik-env.yaml
#+end_src

*** encrypt authentik-env secret
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :results silent :sync
sops --encrypt --in-place ./secrets/authentik-env.yaml
#+end_src
** powerdns-admin
*** get authentik Self-signed certificate
https://sso.uk.sharing.io/if/admin/#/crypto/certificates

* Ghost
** ghost-secret
https://github.com/bitnami/charts/blob/main/bitnami/mysql/values.yaml#L138-L141
#+begin_src shell :prologue "(\nexport KUBECONFIG" :epilogue "\n) 2>&1\n\:\n" :results none :sync
kubectl create secret generic ghost-passwords -n ghost \
    --from-literal=ghost-password=$GHOST_PASSWORD \
    --from-literal=smtp-password=$GHOST_SMTP_PASSWORD \
    --from-literal=mysql-root-password=$GHOST_MYSQL_ROOT_PASSWORD \
    --from-literal=mysql-password=$GHOST_MYSQL_ROOT_PASSWORD \
    -o yaml --dry-run=client > secrets/ghost.yaml
# kubectl apply -f ./ghost-passwords.yaml
#+end_src
** encrypt and commit ghost secret
We need to encrypt the pdns-secret with sops and commit/push so flux can decrypt and apply it
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :results silent :sync
sops --encrypt --in-place secrets/ghost.yaml
#+end_src
** abcs.news-tls
Need to setup another issuer over here... maybe the http one?
#+begin_src shell :results silent
kubectl --kubeconfig=$HOME/.kube/config-sharing.io -n ghost get secrets abcs.news-tls -o yaml > secrets/abcs.news-tls.yaml
#+end_src

** encrypt and commit tls-secret
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :results silent :sync
sops --encrypt --encrypted-regex '^(data|stringData)$' --in-place secrets/abcs.news-tls.yaml
#+end_src
** copy abcs.news-tls from old to new cluster
Need to setup another issuer over here... maybe the http one?
#+begin_src shell :results silent
kubectl --kubeconfig=$HOME/.kube/config-sharing.io -n ghost get secrets abcs.news-tls -o yaml | kubectl --kubeconfig=$HOME/.kube/config-uk.sharing.io apply -f -
#+end_src
** pvcs

Looks like we need to ensure *mount.nfs* is available from nfs-common.

#+begin_example
Events:
  Type     Reason                  Age                 From                     Message
  ----     ------                  ----                ----                     -------
  Normal   Scheduled               23m                 default-scheduler        Successfully assigned ghost/ghost-8499d5fb57-ncqqg to cp1.uk.sharing.io
  Normal   SuccessfulAttachVolume  23m                 attachdetach-controller  AttachVolume.Attach succeeded for volume "pvc-17166eda-0c12-49fe-b264-010dde15a5a3"
  Warning  FailedMount             37s (x19 over 23m)  kubelet                  MountVolume.MountDevice failed for volume "pvc-17166eda-0c12-49fe-b264-010dde15a5a3" : rpc error: code = Internal desc = mount failed: exit status 32
Mounting command: /usr/local/sbin/nsmounter
Mounting arguments: mount -t nfs -o vers=4.1,noresvport,intr,hard 10.98.159.230:/pvc-17166eda-0c12-49fe-b264-010dde15a5a3 /var/lib/kubelet/plugins/kubernetes.io/csi/driver.longhorn.io/fd606d5d5795c1a7eed567e3f011db4aa5f8b475f484e1120cc6af0556f54454/globalmount
Output: mount: /var/lib/kubelet/plugins/kubernetes.io/csi/driver.longhorn.io/fd606d5d5795c1a7eed567e3f011db4aa5f8b475f484e1120cc6af0556f54454/globalmount: bad option; for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
#+end_example

* harbor
https://goharbor.io/docs/1.10/administration/configure-authentication/oidc-auth/

https://harbor.uk.sharing.io/accounts/sign-in
* Footnotes
** LOB
*** defun eval-block
#+begin_src elisp :noweb yes
(defun eval-block (code-block)
  <<eval-block>>
  )
#+end_src

#+RESULTS:
: eval-block

*** eval-block
useful when wanting to asssign results of one block to variables of another
#+name: eval-block
#+begin_src elisp :var code-block="src name" :results silent
(save-excursion (org-babel-goto-named-src-block code-block)
  (let (
        (info (org-babel-get-src-block-info 'light))
        (re-run 't)
        )
    (when info
      (save-excursion
    ;; go to the results, if there aren't any then run the block
    (goto-char (or (and (not re-run) (org-babel-where-is-src-block-result))
               (progn (org-babel-execute-src-block)
                  (org-babel-where-is-src-block-result))))
    (end-of-line 1)
    (while (looking-at "[\n\r\t\f ]")
      (forward-char 1))
    ;; open the results
    (if (looking-at org-link-bracket-re)
        ;; file results
        (org-open-at-point)
      (let ((r (org-babel-format-result
            (org-babel-read-result) (cdr (assq :sep (nth 2 info))))))
        (print! "buffers: %s" (buffer-list))
        (pop-to-buffer (get-buffer-create "*Org-Babel Error*"))
        (print! "error content: %s" (buffer-string))
        (pop-to-buffer (get-buffer-create "load"))
        (print! "Messages content: %s" (buffer-string))
        ;; (pop-to-buffer (get-buffer-create "*Org-Babel Results*"))
        (delete-region (point-min) (point-max))
        (insert r)
        (setq results (buffer-string))))
    (string-trim-right results)))
    ))
;; (ob-eval-block code-block)
#+end_src
** PDNS TSIG Secret
https://github.com/zachomedia/cert-manager-webhook-pdns#powerdns-cert-manager-acme-webhook
Create one here with only access to uk.sharing.io
https://powerdns.ii.nz/admin/manage-keys
*** cert-manager
**** create rfc2136 secret
Note that the TSIG_KEY we retrieve is base64 encoded... it get's double encoded as a kubernetes secret. Most places you use a TSIG_KEY are expecting the base64 value we have here.
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :sync :results silent
source .envrc
kubectl -n cert-manager \
    create secret generic rfc2136 \
    --from-literal="$PDNS_TSIG_KEYNAME"="$PDNS_TSIG_KEY" \
    --dry-run=client -o yaml > \
     ./secrets/cert-manager-rfc2136.yaml
#+end_src

**** encrypt rfc2136 secret
We need to encrypt the pdns-secret with sops and commit/push so flux can decrypt and apply it
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :results silent :sync
sops --encrypt --in-place ./secrets/cert-manager-rfc2136.yaml
#+end_src

*** external-dns
**** create rfc2136 secret
Note that the TSIG_KEY we retrieve is base64 encoded... it get's double encoded as a kubernetes secret. Most places you use a TSIG_KEY are expecting the base64 value we have here.
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\nexport KUBECONFIG\n" :sync :results silent
source .envrc
kubectl -n external-dns \
    create secret generic rfc2136 \
    --from-literal="$PDNS_TSIG_KEYNAME"="$PDNS_TSIG_KEY" \
    --dry-run=client -o yaml > \
    ./secrets/external-dns-rfc2136.yaml
#+end_src

**** encrypt rfc2136 secret
We need to encrypt the pdns-secret with sops and commit/push so flux can decrypt and apply it
#+begin_src shell :epilogue "\n) 2>&1\n:\n" :prologue "(\n" :results silent :sync
sops --encrypt --in-place ./secrets/external-dns-rfc2136.yaml
#+end_src

** Setup SOPS + Flux
*** sops binary
**** linux
#+begin_src shell
wget https://github.com/getsops/sops/releases/download/v3.7.3/sops_3.7.3_amd64.deb
dpkg -i sops_*deb
rm sops_*deb
#+end_src
**** mac
#+begin_src shell
brew install gnupg sops
#+end_src
*** generate gpg key
#+begin_src shell :env KEY_NAME="k8s.uk.sharing.io"
export KEY_NAME="k8s.uk.sharing.io"
export KEY_COMMENT="flux secrets"

gpg --batch --full-generate-key <<EOF
%no-protection
Key-Type: 1
Key-Length: 4096
Subkey-Type: 1
Subkey-Length: 4096
Expire-Date: 0
Name-Comment: ${KEY_COMMENT}
Name-Real: ${KEY_NAME}
EOF
#+end_src

#+begin_src shell :env KEY_NAME="k8s.uk.sharing.io"
export KEY_NAME="k8s.uk.sharing.io"
gpg --list-secret-keys "${KEY_NAME}"
#+end_src

#+end_example
*** import into kubernetes
#+begin_src shell :env KEY_NAME="k8s.uk.sharing.io"
export KEY_NAME="k8s.uk.sharing.io"
export KEY_FP=$(gpg --list-secret-keys "${KEY_NAME}" | grep SCEAR -A1 | tail -1 | awk '{print $1}')
gpg --export-secret-keys --armor "${KEY_FP}" |
kubectl create secret generic sops-gpg \
--namespace=flux-system \
--from-file=sops.asc=/dev/stdin
#+end_src

#+RESULTS:
#+begin_example
secret/sops-gpg created
#+end_example
*** export key into git
#+begin_src shell :env KEY_NAME="k8s.uk.sharing.io" :results silent
export KEY_NAME="k8s.uk.sharing.io"
export KEY_FP=$(gpg --list-secret-keys "${KEY_NAME}" | grep SCEAR -A1 | tail -1 | awk '{print $1}')
gpg --export --armor "${KEY_FP}" > ./.sops.pub.asc
#+end_src

*** write SOPS config file
#+begin_src shell :env KEY_NAME="k8s.uk.sharing.io" :results silent
export KEY_NAME="k8s.uk.sharing.io"
export KEY_FP=$(gpg --list-secret-keys "${KEY_NAME}" | grep SCEAR -A1 | tail -1 | awk '{print $1}')
cat <<EOF > .sops.yaml
creation_rules:
  - path_regex: .*.yaml
    encrypted_regex: ^(data|stringData)$
    pgp: ${KEY_FP}
EOF
#+end_src

https://fluxcd.io/flux/guides/mozilla-sops/#prerequisites
** Create .envrc
*** Example
#+begin_src shell :tangle .envrc.example :tangle-mode (identity #o600)
# Just needs to be a secrets `pwgen 12` is fine
export CODER_DB_PASSWORD=
export CODER_PG_CONNECTION_URL=postgres://postgres:$CODER_DB_PASSWORD@coder-db-postgres.coder.svc.cluster.local:5432/coder?sslmode=disabled
# We Could set a GITHUB Token for coder, but not for now
# https://github.com/settings/personal-access-tokens/new
# https://github.com/settings/personal-access-tokens/1566846
export GITHUB_TOKEN=
# https://coder.com/docs/v2/latest/admin/auth
# https://github.com/organizations/cloudnative-nz/settings/applications/new
# https://github.com/organizations/cloudnative-nz/settings/applications/2247328
export CODER_OAUTH2_GITHUB_CLIENT_ID=
export CODER_OAUTH2_GITHUB_CLIENT_SECRET=
# https://coder.com/docs/v2/latest/admin/git-providers
# https://github.com/organizations/cloudnative-nz/settings/apps/new
# https://github.com/organizations/cloudnative-nz/settings/apps/space-cloudnative-nz
# NOTE: this is NOT the App ID, use Client ID
export CODER_GITAUTH_0_CLIENT_ID=
# NOTE: this is Client Secret
export CODER_GITAUTH_0_CLIENT_SECRET=
# We created a tsig key with DNS-UPDATE only for uk.sharing.io
export PDNS_API_KEY=
# I created a new project a key
# https://console.equinix.com/projects/0c218738-18c0-47b5-a404-beb59d9c6585/general
export METAL_PROJECT_ID=
# https://console.equinix.com/projects/0c218738-18c0-47b5-a404-beb59d9c6585/api-keys
export METAL_AUTH_TOKEN=
# OIDC with authentik deployed to sso.ii.nz
# https://sso.ii.nz/if/admin/#/core/applications/cloudnative-nz
# https://sso.ii.nz/if/admin/#/core/providers/2
export CODER_OIDC_CLIENT_ID=
export CODER_OIDC_CLIENT_SECRET=
#+end_src
*** Save .envrc to .enc.envrc with sops
#+begin_src tmux :session "cluster:secret"
sops -e --input-type dotenv .envrc > .enc.envrc
#+end_src
** Create Kubernetes Secrets
*** Save pdns TSIG key as a kubernetes secret
PDNS_TSIG_KEY needs to be set to the activated TSIG key with TSIG-ALLOW_DNSUPDATE
#+begin_src tmux :session "cluster:secret"
. .envrc
echo $PDNS_TSIG_KEY
kubectl create secret generic pdns \
    --namespace=cert-manager \
    --from-literal=key=$PDNS_TSIG_KEY \
    -o yaml \
    --dry-run=client > ./pdns-secret.yaml
sops --encrypt --in-place pdns-secret.yaml
#+end_src

*** generate coder secret
Mostly we map the env vars directly.
#+begin_src bash :sync :results silent :dir .
. .envrc
kubectl create secret generic coder \
    --namespace=coder \
    --from-literal=TUNNELD_WIREGUARD_KEY=$TUNNELD_WIREGUARD_KEY \
    --from-literal=PDNS_TSIG_KEY=$PDNS_TSIG_KEY \
    --from-literal=PDNS_API_KEY=$PDNS_API_KEY \
    --from-literal=GITHUB_TOKEN=$GITHUB_TOKEN \
    --from-literal=CODER_USERNAME=$CODER_USERNAME \
    --from-literal=CODER_PASSWORD=$CODER_PASSWORD \
    --from-literal=CODER_OAUTH2_GITHUB_CLIENT_ID=$CODER_OAUTH2_GITHUB_CLIENT_ID \
    --from-literal=CODER_OAUTH2_GITHUB_CLIENT_SECRET=$CODER_OAUTH2_GITHUB_CLIENT_SECRET \
    --from-literal=CODER_GITAUTH_0_CLIENT_ID=$CODER_GITAUTH_0_CLIENT_ID \
    --from-literal=CODER_GITAUTH_0_CLIENT_SECRET=$CODER_GITAUTH_0_CLIENT_SECRET \
    --from-literal=CODER_OIDC_CLIENT_ID=$CODER_OIDC_CLIENT_ID \
    --from-literal=CODER_OIDC_CLIENT_SECRET=$CODER_OIDC_CLIENT_SECRET \
    --from-literal=METAL_AUTH_TOKEN=$METAL_AUTH_TOKEN \
    --from-literal=password=$CODER_DB_PASSWORD \
    --from-literal=postgres-password=$CODER_DB_PASSWORD \
    --from-literal=CODER_PG_CONNECTION_URL="postgres://postgres:$CODER_DB_PASSWORD@coder-db-postgresql.coder.svc.cluster.local:5432/coder?sslmode=disable" \
    -o yaml --dry-run=client > ./secrets/coder.yaml
#+end_src

*** encode coder secret
Mostly we map the env vars directly.
#+begin_src bash :sync :results silent :dir .
sops --encrypt --in-place ./secrets/coder.yaml
#+end_src
*** describe coder secret
Mostly we map the env vars directly.
#+begin_src bash :sync :dir .
kubectl -n coder describe secret coder
#+end_src

#+RESULTS:
#+begin_example
Name:         coder
Namespace:    coder
Labels:       kustomize.toolkit.fluxcd.io/name=secrets
              kustomize.toolkit.fluxcd.io/namespace=flux-system
Annotations:  <none>

Type:  Opaque

Data
====
CODER_OIDC_CLIENT_ID:               40 bytes
GITHUB_TOKEN:                       93 bytes
METAL_AUTH_TOKEN:                   32 bytes
PDNS_API_KEY:                       8 bytes
CODER_GITAUTH_0_CLIENT_ID:          20 bytes
CODER_GITAUTH_0_CLIENT_SECRET:      40 bytes
CODER_OAUTH2_GITHUB_CLIENT_ID:      20 bytes
CODER_OAUTH2_GITHUB_CLIENT_SECRET:  40 bytes
postgres-password:                  12 bytes
CODER_OIDC_CLIENT_SECRET:           128 bytes
CODER_PG_CONNECTION_URL:            103 bytes
PDNS_TSIG_KEY:                      88 bytes
password:                           12 bytes
#+end_example

** Variables
# Local Variables:
# space-domain: "uk.sharing.io"
# tramp-dir: "/ssh:root@uk.sharing.io:/"
# eval: (setq org-babel-tmux-terminal "kitty")
# eval: (setq org-babel-tmux-terminal-opts '("--hold"))
# End:
